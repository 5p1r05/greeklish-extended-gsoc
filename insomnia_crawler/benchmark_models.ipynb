{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scores for misaligned texts\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Checks whether the majority of the letters in the input text are in the greek or the latin script\n",
    "    It is used to identify whether the text is in greek or greeklish (latin script), in order to skip unnecessary conversions.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text\n",
    "\n",
    "    Returns:\n",
    "        script (str): The dominant script\n",
    "    \"\"\"\n",
    "    # Filter out non-letter characters\n",
    "    valid_characters = [char for char in text if char.isalpha()]\n",
    "    \n",
    "    # Count Greek and English letters\n",
    "    greek_count = sum(1 for char in valid_characters if '\\u0370' <= char <= '\\u03FF' or '\\u1F00' <= char <= '\\u1FFF')\n",
    "    english_count = sum(1 for char in valid_characters if '\\u0041' <= char <= '\\u005A' or '\\u0061' <= char <= '\\u007A')\n",
    "    \n",
    "    if(greek_count == 0 and english_count == 0):\n",
    "        return \"unknown\"\n",
    "\n",
    "    script = \"greek\" if greek_count >= english_count else \"latin\"\n",
    "    return script\n",
    "\n",
    "def calculate_scores_misaligned(original_text, predicted_text, gt_indices):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    \n",
    "    FP_words = []\n",
    "    FN_words = []\n",
    "\n",
    "    predicted_text = predicted_text.lower()\n",
    "    \n",
    "    indices_not_found = []\n",
    "    for index in gt_indices:\n",
    "        word = original_text.split(\" \")[index].lower()\n",
    "        \n",
    "        # Check whether the word is in the predicted text\n",
    "        if(word in predicted_text):\n",
    "            TP += 1\n",
    "        # If the word is not in the predicted text, it is a false negative\n",
    "        else:\n",
    "            FN += 1\n",
    "            indices_not_found.append(index)\n",
    "            FN_words.append(word)\n",
    "\n",
    "    # Check for false positives\n",
    "    original_english_words = [original_text.split(\" \")[index].lower() for index in gt_indices]\n",
    "    for i, word in enumerate(predicted_text.split(\" \")):\n",
    "        # only keep the letters\n",
    "        stripped_word = \" \".join(re.findall(\"[a-zA-Z]+\", word))\n",
    "        if detect_language(stripped_word) == \"latin\":\n",
    "            if(word not in original_english_words):\n",
    "                FP += 1\n",
    "                FP_words.append(word)\n",
    "    \n",
    "    return TP, FP, FN, FP_words, FN_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_llm(path=\"LLMs/LLM_data/llama-3.1-70b-versatile_0.0_data/\"):\n",
    "    llama_data = os.listdir(path)[7:8]\n",
    "    print(llama_data)\n",
    "\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    TP_all = 0\n",
    "    FP_all = 0\n",
    "    FN_all = 0\n",
    "\n",
    "    FP_words_all = []\n",
    "    FN_words_all = []\n",
    "\n",
    "    for forum in llama_data:\n",
    "        with open(path + forum, \"r\") as f:\n",
    "            text_data = json.load(f)\n",
    "        \n",
    "        # with open(\"forums_info/forums_sampled/\" + forum, \"r\") as f:\n",
    "        #     annotations_data = json.load(f)\n",
    "\n",
    "        for annotation in text_data:\n",
    "            original_text = annotation[\"greeklish\"]\n",
    "            gt_indices = annotation['gt_indices']\n",
    "            \n",
    "                \n",
    "            predicted_text = annotation[\"greek\"]\n",
    "\n",
    "            TP, FP, FN, FP_words, FN_words  = calculate_scores_misaligned(original_text, predicted_text, gt_indices)\n",
    "            \n",
    "            FP_words_all.extend(FP_words)\n",
    "            FN_words_all.extend(FN_words)\n",
    "\n",
    "            # skip if the denominator is 0\n",
    "            if(TP + FN == 0 or TP + FP == 0):\n",
    "                continue\n",
    "\n",
    "            TP_all += TP\n",
    "            FP_all += FP\n",
    "            FN_all += FN\n",
    "\n",
    "            recall = TP / (TP + FN)\n",
    "            precision = TP / (TP + FP)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "\n",
    "    micro_average_precision = TP_all / (TP_all + FP_all)\n",
    "    micro_average_recall = TP_all / (TP_all + FN_all)\n",
    "\n",
    "    macro_average_precision = sum(precisions) / len(precisions)\n",
    "    macro_average_recall = sum(recalls) / len(recalls)\n",
    "    \n",
    "    print(\"FP_words: \", FP_words_all)\n",
    "    print(\"FN_words: \", FN_words_all)\n",
    "    \n",
    "    return micro_average_precision, micro_average_recall, macro_average_precision, macro_average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_text = {\n",
    "#     \"text\" : \"prepei na kanoume adjust sta kainouria guidelines\",\n",
    "#     \"gt_indices\": [3, 6]\n",
    "# }\n",
    "\n",
    "# predicted_text = \"πρεπει να κανουμε adjust στα kainourgia guidelines\"\n",
    "\n",
    "# recall, precision = calculate_scores_misaligned(original_text[\"text\"], predicted_text, original_text[\"gt_indices\"])\n",
    "# print(\"Recall: \", recall)\n",
    "# print(\"Precision: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def benchmark_llm(path=\"LLMs/LLM_data/llama-3.1-70b-versatile_0.0_data/\"):\n",
    "    llama_data = os.listdir(path)\n",
    "    print(llama_data)\n",
    "\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    TP_all = 0\n",
    "    FP_all = 0\n",
    "    FN_all = 0\n",
    "\n",
    "    FP_words_all = []\n",
    "    FN_words_all = []\n",
    "\n",
    "    for forum in llama_data:\n",
    "        with open(path + forum, \"r\") as f:\n",
    "            text_data = json.load(f)\n",
    "        \n",
    "        with open(\"forums_info/forums_sampled/\" + forum, \"r\") as f:\n",
    "            annotations_data = json.load(f)\n",
    "\n",
    "        for i, annotation in enumerate(annotations_data):\n",
    "            original_text = annotation[\"text\"]\n",
    "            gt_indices = annotation[\"gt_indices\"]\n",
    "            # check if the annotation file is aligned with the LLM data\n",
    "            if(text_data[i][\"greeklish\"] != original_text):\n",
    "                print(\"text not found (misalignment)\")\n",
    "                exit(0)\n",
    "                \n",
    "            predicted_text = text_data[i][\"greek\"]\n",
    "\n",
    "            TP, FP, FN, FP_words, FN_words  = calculate_scores_misaligned(original_text, predicted_text, gt_indices)\n",
    "            \n",
    "            FP_words_all.extend(FP_words)\n",
    "            FN_words_all.extend(FN_words)\n",
    "\n",
    "            # skip if the denominator is 0\n",
    "            if(TP + FN == 0 or TP + FP == 0):\n",
    "                continue\n",
    "\n",
    "            TP_all += TP\n",
    "            FP_all += FP\n",
    "            FN_all += FN\n",
    "\n",
    "            recall = TP / (TP + FN)\n",
    "            precision = TP / (TP + FP)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "\n",
    "    micro_average_precision = TP_all / (TP_all + FP_all)\n",
    "    micro_average_recall = TP_all / (TP_all + FN_all)\n",
    "\n",
    "    macro_average_precision = sum(precisions) / len(precisions)\n",
    "    macro_average_recall = sum(recalls) / len(recalls)\n",
    "    \n",
    "    print(\"FP_words: \", FP_words_all)\n",
    "    print(\"FN_words: \", FN_words_all)\n",
    "    \n",
    "    return micro_average_precision, micro_average_recall, macro_average_precision, macro_average_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Διαδίκτυο_sample.json']\n",
      "FP_words:  ['chants.', 'κατέβazeis', 'kazaa', 'mclaren;']\n",
      "FN_words:  ['folder', 'chans.', 'kazza', 'mclaren?']\n",
      "Average macro precision: 0.95\n",
      "Average macro recall: 0.96\n",
      "Average micro precision: 0.95\n",
      "Average micro recall: 0.95\n"
     ]
    }
   ],
   "source": [
    "    # Benchmark LLama\n",
    "    \n",
    "    micro_average_precision, micro_average_recall, macro_average_precision, macro_average_recall = benchmark_llm(\"LLMs/LLM_data/llama-3.1-70b-versatile_0.0_data/\")\n",
    "    \n",
    "    print(f\"Average macro precision: {macro_average_precision:.2f}\")\n",
    "    print(f\"Average macro recall: {macro_average_recall:.2f}\")\n",
    "\n",
    "   \n",
    "\n",
    "    print(f\"Average micro precision: {micro_average_precision:.2f}\")\n",
    "    print(f\"Average micro recall: {micro_average_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gadgets_sample.json']\n",
      "FP_words:  ['plz', 's60', 'p800,900', 'vga@30φπσ', 'ir', 'ir', 't610)', 'κεραμίδαςmob.', 'μοντέλο(nokia,', 'se),']\n",
      "FN_words:  ['melodies', 'nokia', 'nokia!', 'sony', 'ericsson', 'vivaz------------------------------------------------------------------------', 'sony', 'ericsson', 'aino', 'video', 'yes,', '720p@24fps,', 'continuous', 'autofocus,', 'video', 'light------------------------------------------', 'vga@30fps', 'voda']\n",
      "Average macro precision: 0.78\n",
      "Average macro recall: 0.85\n",
      "Average micro precision: 0.85\n",
      "Average micro recall: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Benchmark GPT-4o\n",
    "\n",
    "micro_average_precision, micro_average_recall, macro_average_precision, macro_average_recall = benchmark_llm(\"LLMs/LLM_data/gpt-4o_0.0_data/\")\n",
    "print(f\"Average macro precision: {macro_average_precision:.2f}\")\n",
    "print(f\"Average macro recall: {macro_average_recall:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Average micro precision: {micro_average_precision:.2f}\")\n",
    "print(f\"Average micro recall: {micro_average_recall:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_greeklish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
